import scrapy
from daum_it.items import DaumItItem
from datetime import datetime, timedelta

class DaumItSpiderSpider(scrapy.Spider):
    name = "daum_it_spider"

    # 날짜 범위 설정 (현재 날짜부터 이전 3개월까지)
    end_date = datetime.now()
    start_date = end_date - timedelta(days=90)

    # 카테고리 목록 설정
category_list = ["internet", "science", "game", "it", "device", "mobile", "software", "others"]
for category in category_list:
    #start_urls = ["http://news.daum.net//breakingnews/digital?regDate=20230904"]

    def start_requests(self):
            global url #전역변수설정

            # 날짜 범위 내의 날짜를 반복
            current_date = self.start_date
            while current_date <= self.end_date:
                formatted_date = current_date.strftime("%Y%m%d")
                for page_num in range(1,):  # 페이지 번호를 조절하세요 (1부터 끝까지)
                    url = f"https://news.daum.net/breakingnews/digital/{category}?page={page_num}&regDate={formatted_date}"
                    yield scrapy.Request(url, callback=self.parse)

                # 다음 날짜로 이동
                current_date += timedelta(days=1)


    def parse(self, response):

        # global URL

        for i in range(1,16):
            URL = response.xpath(f'//*[@id="mArticle"]/div[3]/ul/li[{i}]/div/strong/a/@href')[0].extract() #본문 URL
            yield scrapy.Request(URL,callback=self.parse_page_content1)
            #print(URL,i)
            # div = response.xpath(f'//*[@id="mArticle"]/div[3]/ul/li[{i}]')

            # if (URL !=[]): #i가 비어있지않다면
            #     href = div.xpath('./div/strong/a/@href')
            #     url = response.urljoin(href[0].extract())
            #     print(url)

    def parse_page_content1(self,response): #세부항목들안으로 들어가는 함수
        item = DaumItItem()

        item['Title'] = response.xpath('//*[@id="mArticle"]/div[1]/h3/text()')[0].extract()
        item['Date'] = response.xpath('//*[@id="mArticle"]/div[1]/div[1]/span[2]/span/text()')[0].extract()
        #item['수정일시']
        item['Media'] = response.xpath('//*[@id="kakaoServiceLogo"]/text()')[0].extract()
        item['Content_URL'] = url#수정필요->전역변수 설정변경해야할듯
        item['Contents'] = response.xpath('//*[@id="mArticle"]/div[2]/div[2]/section/p/text()').getall()
        item['Pressname'] = response.xpath('//*[@id="mArticle"]/div[1]/div[1]/span[1]/text()')[0].extract()
        item['Category'] = category
        return item